# -*- coding: utf-8 -*-
"""Assigment - Day 21 - PySpark.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1HJwhCnYxOrQF7fDNvvPV8ZIOHDVfdsgl

# Preparation
"""

# Install pyspark
!pip install pyspark

from pyspark.sql import SparkSession

spark = SparkSession.builder \
    .appName("AssigmentPySpark") \
    .getOrCreate()

df1 = spark.read.csv("calendar.csv", header=True, inferSchema=True)
df2 = spark.read.csv("customer_flight_activity.csv", header=True, inferSchema=True)
df3 = spark.read.csv("customer_loyalty_history.csv", header=True, inferSchema=True)

df1.printSchema()
df2.printSchema()
df3.printSchema()

"""# **Data Cleaning**"""

from pyspark.sql.functions import col, trim, regexp_replace

# ====== DF1: calendar.csv ======
# 1. Hapus baris kosong dan duplikat
df1 = df1.na.drop(how='all').dropDuplicates()

# ====== DF2: customer_flight_activity.csv ======
df2 = df2.na.drop(how='all').dropDuplicates()

# 2. Pastikan tipe data numerik sudah benar
df2 = df2.withColumn("loyalty_number", col("loyalty_number").cast("int")) \
         .withColumn("year", col("year").cast("int")) \
         .withColumn("month", col("month").cast("int")) \
         .withColumn("total_flights", col("total_flights").cast("int")) \
         .withColumn("distance", col("distance").cast("int")) \
         .withColumn("points_accumulated", col("points_accumulated").cast("double")) \
         .withColumn("points_redeemed", col("points_redeemed").cast("int")) \
         .withColumn("dollar_cost_points_redeemed", col("dollar_cost_points_redeemed").cast("int"))

# ====== DF3: customer_loyalty_history.csv ======
df3 = df3.na.drop(how='all').dropDuplicates()

# 3. Trim spasi berlebih dari kolom string
string_cols = ["country", "province", "city", "postal_code", "gender", "education",
               "marital_status", "loyalty_card", "customer_lifetime_value",
               "enrollment_type", "enrollment_year", "enrollment_month"]

for col_name in string_cols:
    df3 = df3.withColumn(col_name, trim(col(col_name)))

# 4. Perbaiki format nilai numerik
df3 = df3.withColumn("salary", col("salary").cast("double")) \
         .withColumn("cancellation_year", col("cancellation_year").cast("int")) \
         .withColumn("cancellation_month", col("cancellation_month").cast("int"))

# 5. Ubah customer_lifetime_value jadi numerik (hapus simbol $, koma)
df3 = df3.withColumn("customer_lifetime_value",
                     regexp_replace("customer_lifetime_value", "[$,]", "").cast("double"))

df1.printSchema()
df2.printSchema()
df3.printSchema()

df1.show(3)
df2.show(3)
df3.show(3)

"""# **Transformasi Data**"""

# Join df2 (flight activity) dengan df3 (customer info) berdasarkan 'loyalty_number'
joined_df = df2.join(df3, on="loyalty_number", how="inner")

# Buat kolom 'activity_date' dari df2 (gunakan calendar df1 untuk dapatkan tanggal)
joined_df = joined_df.join(
    df1.select("date", "start_of_the_month"),
    (df1["date"].substr(1, 4) == joined_df["year"].cast("string")) &
    (df1["date"].substr(6, 2) == joined_df["month"].cast("string")),
    how="left"
)

from pyspark.sql.functions import sum

# Total poin dan total distance per tahun
aggregated_df = df2.groupBy("year").agg(
    sum("points_accumulated").alias("total_points"),
    sum("distance").alias("total_distance"),
    sum("total_flights").alias("total_flights")
).orderBy("year")

from pyspark.sql.functions import col

# Buat kolom total poin tahunan per customer
customer_yearly_points = df2.groupBy("loyalty_number", "year").agg(
    sum("points_accumulated").alias("yearly_total_points")
)

joined_df = joined_df.join(customer_yearly_points, on=["loyalty_number", "year"], how="left")

aggregated_df.show()
customer_yearly_points.show()
joined_df.select("loyalty_number", "year", "yearly_total_points", "gender", "education").show(5)

"""# **PySpark SQL Usage**"""

#Create View terlebih dahulu
df2.createOrReplaceTempView("flight_activity")
df3.createOrReplaceTempView("customer_info")

#Rata-rata jumlah penerbangan per pelanggan dalam setahun

avg_flights = spark.sql("""
    SELECT
        AVG(total_flights_per_year) AS avg_flights_per_customer_per_year
    FROM (
        SELECT
            loyalty_number,
            year,
            SUM(total_flights) AS total_flights_per_year
        FROM flight_activity
        GROUP BY loyalty_number, year
    )
""")
avg_flights.show()

# Distribusi loyalty points berdasarkan status kartu loyalitas

# Join untuk dapat akses 'loyalty_card'
# Ambil kolom yang relevan dari df3
df3_selected = df3.select("loyalty_number", "loyalty_card", "education")

# Join dengan df2
joined = df2.join(df3_selected, on="loyalty_number", how="inner")

# Register ulang view
joined.createOrReplaceTempView("joined_data")


spark.sql("""
    SELECT
        loyalty_card,
        AVG(points_accumulated) AS avg_points,
        MIN(points_accumulated) AS min_points,
        MAX(points_accumulated) AS max_points
    FROM joined_data
    GROUP BY loyalty_card
""").show()

# Hubungan antara tingkat pendidikan dengan jumlah penerbangan
spark.sql("""
    SELECT
        education,
        AVG(total_flights) AS avg_flights
    FROM joined_data
    GROUP BY education
    ORDER BY avg_flights DESC
""").show()

# Tren jumlah penerbangan dari waktu ke waktu

spark.sql("""
    SELECT
        year,
        month,
        SUM(total_flights) AS monthly_total_flights
    FROM flight_activity
    GROUP BY year, month
    ORDER BY year, month
""").show()

"""# **Visualisasi Data**"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# 1. Tren jumlah penerbangan dari waktu ke waktu
monthly_flights = spark.sql("""
    SELECT year, month, SUM(total_flights) AS monthly_total_flights
    FROM flight_activity
    GROUP BY year, month
    ORDER BY year, month
""").toPandas()

monthly_flights["date"] = pd.to_datetime(monthly_flights["year"].astype(str) + "-" + monthly_flights["month"].astype(str) + "-01")

# 2. Distribusi loyalty points berdasarkan loyalty card
points_dist = spark.sql("""
    SELECT loyalty_card, AVG(points_accumulated) AS avg_points
    FROM joined_data
    GROUP BY loyalty_card
""").toPandas()

# 3. Rata-rata penerbangan berdasarkan tingkat pendidikan
edu_avg_flights = spark.sql("""
    SELECT education, AVG(total_flights) AS avg_flights
    FROM joined_data
    GROUP BY education
    ORDER BY avg_flights DESC
""").toPandas()

# --- Buat Visualisasi ---
plt.figure(figsize=(18, 12))

# Grafik 1: Tren jumlah penerbangan
fig1, ax1 = plt.subplots(figsize=(10, 4))
sns.lineplot(data=monthly_flights, x="date", y="monthly_total_flights", marker='o', color='blue', ax=ax1)
ax1.set_title("Tren Jumlah Penerbangan Bulanan")
ax1.set_xlabel("Tanggal")
ax1.set_ylabel("Jumlah Penerbangan")
ax1.grid(True)

# Grafik 2: Loyalty points berdasarkan jenis kartu
fig2, ax2 = plt.subplots(figsize=(10, 6))
points_sorted = points_dist.sort_values("avg_points", ascending=False)
sns.barplot(data=points_sorted, x="loyalty_card", y="avg_points", palette="coolwarm", ax=ax2)
ax2.set_title("Rata-rata Loyalty Points per Loyalty Card")
ax2.set_xlabel("Jenis Kartu Loyalty")
ax2.set_ylabel("Rata-rata Poin")
ax2.set_ylim(0, points_sorted["avg_points"].max() * 1.1)
ax2.grid(axis='y')

# Tambahkan label angka di atas batang
for index, row in points_sorted.iterrows():
    ax2.text(index, row.avg_points + 5, f"{row.avg_points:.1f}", ha='center', va='bottom')

# Grafik 3: Penerbangan rata-rata berdasarkan pendidikan
fig3, ax3 = plt.subplots(figsize=(12, 6))
edu_sorted = edu_avg_flights.sort_values("avg_flights", ascending=False)
sns.barplot(data=edu_sorted, x="education", y="avg_flights", palette="viridis", ax=ax3)
ax3.set_title("Rata-rata Penerbangan per Tahun Berdasarkan Pendidikan")
ax3.set_xlabel("Tingkat Pendidikan")
ax3.set_ylabel("Rata-rata Penerbangan")
ax3.set_ylim(0, edu_sorted["avg_flights"].max() * 1.1)
ax3.set_xticklabels(ax3.get_xticklabels(), rotation=45)
ax3.grid(axis='y')

plt.show()

"""# **Download Hasil Analisis**"""

monthly_flights.to_csv("/content/monthly_flights.csv", index=False)

points_dist.to_csv("/content/point_distance.csv", index=False)

edu_avg_flights.to_csv("/content/edu_avg_flights.csv", index=False)

fig1.savefig("tren_penerbangan.png")
fig2.savefig("loyalty_points.png")
fig3.savefig("rata_penerbangan_pendidikan.png")

# Isi konten markdown-nya
content = """
# Assigment - Day 21 - PySpark.

Phase 1 : Ekstrak Data ke dalam DataFrame
- Muat dataset ke dalam PySpark DataFrame menggunakan "spark.read.csv"
- Periksa tipe data setiap kolom dan sesuaikan jika diperlukan menggunakan "printSchema()"

Phase 2 : Cleaning Data
- Tangani missing values dengan metode yang sesuai menggunakan ".na.drop"
- Hapus data duplikat jika ditemukan menggunakan ".dropDuplicates"
- Perbaiki format data yang tidak sesuai menggunakan "trim" dan "regexp_replace"

Phase 3 : Transformasi Data
- Menggunakan JOIN antar tabel df2 dan df3 untuk mendapatkan informasi yang lebih detail.
- Buat kolom baru yaitu customer_yearly_points

Phase 4: Analisis Data Menggunakan PySpark SQL
- Membuat temporary view untuk setiap DataFrame
- Menganalisa rata-rata jumlah penerbangan per pelanggan dalam setahun
    dengan Query SQL:

     SELECT
        AVG(total_flights_per_year) AS avg_flights_per_customer_per_year
    FROM (
        SELECT
            loyalty_number,
            year,
            SUM(total_flights) AS total_flights_per_year
        FROM flight_activity
        GROUP BY loyalty_number, year

- Menganalisa distribusi loyalty points berdasarkan status kartu loyalitas
    dengan Query SQL:

     SELECT
        loyalty_card,
        AVG(points_accumulated) AS avg_points,
        MIN(points_accumulated) AS min_points,
        MAX(points_accumulated) AS max_points
    FROM joined_data
    GROUP BY loyalty_card

- Menganalisa hubungan antara tingkat pendidikan pelanggan dengan jumlah penerbangan yang mereka lakukan
    dengan Query SQL:

    SELECT
        education,
        AVG(total_flights) AS avg_flights
    FROM joined_data
    GROUP BY education
    ORDER BY avg_flights DESC


- Menganalisa tren jumlah penerbangan dari waktu ke waktu
    dengan Query SQL:

     SELECT
        year,
        month,
        SUM(total_flights) AS monthly_total_flights
    FROM flight_activity
    GROUP BY year, month
    ORDER BY year, month


Phase 5 : Visualisasi Data
- Memvisualisasikan Tren jumlah penerbangan dari waktu ke waktu dengan kesimpulan bahwa penerbangan terbanyak ada di bulan Juli tahun 2018 dan penerbangan terendah ada di bulan Januari tahun 2017
- Memvisualisasikan Distribusi loyalty points berdasarkan loyalty card dengan kesimpulan loyalty card "Aurora" mempunyai rata-rata loyalty points tertinggi dibandingkan dengan loyalty cards lainnya
- Memvisualisasikan Rata-rata penerbangan berdasarkan tingkat pendidikan dengan kesimpulan bahwa Penerbangan terbanyak per tahun ada di tingkat pendidikan "Highschool or Below"
"""

# Simpan ke file .md
with open("README.md", "w") as file:
    file.write(content)